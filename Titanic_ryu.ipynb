{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport re\nimport math\nimport matplotlib.pyplot as plt\nfilepath = \"/kaggle/input/titanic/\"\nsample = pd.read_csv(filepath + \"gender_submission.csv\")\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set = pd.read_csv(filepath + \"test.csv\")\ntest_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = pd.read_csv(filepath + \"train.csv\")\ntraining_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multiple_replace(letters, text):\n  # Create a regular expression  from the dictionary keys\n  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, letters.keys())))\n\n  # For each match, look-up corresponding value in dictionary\n  return regex.sub(lambda mo: letters[mo.string[mo.start():mo.end()]], text) \n\n#Preprocesses data by substituting nonnumerical data and filling in Null data. \n#Additionally drops columns that are not important\ndef preprocess(keys, training_set):\n    ageMean = training_set['Age'].mean()\n    fareMean = training_set['Fare'].mean()\n    training_set['Age'].fillna(ageMean, inplace=True)\n    training_set['Fare'].fillna(fareMean, inplace = True) \n    training_set['Sex'] = training_set['Sex'].map({'female': 0, 'male': 1})\n    training_set['Embarked'] = training_set['Embarked'].map({'S': 0, 'Q': 1, 'C':2})\n    training_set['Embarked'].fillna(3, inplace = True) \n    count = 0\n    for row in training_set['Ticket']:\n        training_set.loc[count, 'Ticket'] = re.sub('[^0-9]','', row)\n        training_set['Ticket'].replace({\"\": 0}, inplace=True)\n        count+=1\n    \n    count = 0\n    for row in training_set['Cabin']:\n        if isinstance(row, float):\n            training_set.loc[count, 'Cabin']=0.0\n        else:\n            training_set.loc[count, 'Cabin'] = float(multiple_replace(letters,row))\n        count+=1\n    training_set.drop(['PassengerId','Name'], axis=1, inplace=True)\n    return training_set\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stores ids for later\nids = test_set['PassengerId']\nletters = {\n    \" \" : \"\",\n    \"A\" : \"1\",\n    \"B\" : \"2\",\n    \"C\" : \"3\",\n    \"D\" : \"4\",\n    \"E\" : \"5\",\n    \"F\" : \"6\",\n    \"G\" : \"7\",\n    \"T\" : \"0\"\n  } \ntraining_set = preprocess(letters, training_set)\ntest_set = preprocess(letters, test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#heatmap just to show possible important features\nsns.set()\ncorrelation = training_set.corr()\nplt.figure(figsize=(15,15))\nautoMatrix = sns.heatmap(\n    correlation, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(0, 200, n=300),\n    annot=True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(training_set.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_set.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = training_set.drop(['Survived'], axis=1)\ny_train = training_set['Survived']\nx_test = test_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import GridSearchCV\nx_train.shape, y_train.shape, x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nmin_max = MinMaxScaler()\nx_train_std = scaler.fit_transform(x_train)\nx_test_std = scaler.fit_transform(x_test)\nx_train_norm = min_max.fit_transform(x_train)\nx_test_norm = min_max.fit_transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = MLPClassifier(random_state = 3, early_stopping=True, verbose=2, learning_rate='adaptive', solver='adam')\nparameters = {\"batch_size\": [1, 10, 25],\n             \"hidden_layer_sizes\" : [(10), (10, 2), (10,10)],\n             }\nmlp = GridSearchCV(model, parameters, cv=6)\nmlp.fit(x_train_norm, y_train)\nprint(\"Best values\", mlp.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ntrainPred = mlp.predict(x_train_norm)\ntarget_names = ['0', '1']\nprint(\"                         MLP Training Report\", '\\n')\nprint(classification_report(y_train, trainPred, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nparameters = {\"C\" : [0.01, 0.1, 1, 1.5, 2, 5, 10],\n             \"gamma\" : [0.01, 0.1, 0.2, 0.3, 0.5, 0.75],\n             \"kernel\" : ['rbf', 'sigmoid']}\nmodel = SVC()\nsvm = GridSearchCV(model, parameters, cv=10)\nsvm.fit(x_train_std, y_train)\nprint(\"Best values\", svm.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ntrainPred = svm.predict(x_train_std)\ntarget_names = ['0', '1']\nprint(\"                         SVM Training Report\", '\\n')\nprint(classification_report(y_train, trainPred, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform, truncnorm, randint\n# parameters = {\"n_estimators\" : [25, 50, 100],\n#              \"max_depth\" : [10, 15, 25],\n#               \"max_leaf_nodes\": [25, 50, 100],\n#              \"min_samples_leaf\" : [1, 2, 5]}\nparameters = {\"n_estimators\" : randint(25,200),\n             \"max_depth\" : randint(5,30),\n              \"max_leaf_nodes\": randint(20,100),\n             \"min_samples_leaf\": randint(1, 5)}\nrf = RandomForestClassifier(n_jobs = -1)\nforest = RandomizedSearchCV(rf, param_distributions=parameters, n_iter=50, cv=7)\n# forest = GridSearchCV(rf, parameters, cv=10)\nforest.fit(x_train, y_train)\nprint(\"Best values\", forest.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ntrainPred = forest.predict(x_train)\ntarget_names = ['0', '1']\nprint(\"                         RF Training Report\", '\\n')\nprint(classification_report(y_train, trainPred, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Make predictions using the features from the test data set\n#mlp, svm, or forest\npredictions = forest.predict(x_test)\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\ny_pred = forest.predict(x_train)\n# y_pred = svm.predict(x_train_std)\ncm = confusion_matrix(y_train, y_pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'PassengerId':ids,'Survived':predictions})\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = 'Titanic Predictions 1.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}